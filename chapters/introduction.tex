%!TEX root = ../dissertation.tex
\chapter{Introduction}
\label{introduction}

\newthought{How can we build} theoretically satisfying and practically useful models of human cognition? Historically, there have been two broad approaches. The \emph{rational} approach, exemplified by the work of David Marr (e.g., \citeyear{marr1982vision}) and John Anderson (e.g., \citeyear{anderson1990adaptive}), begins by characterizing the problems people have to solve and then assumes that their cognitive systems will be optimally designed to solve those problems. Rational models are satisfying because they tell us \emph{why} the mind works the way it does, and they are useful because they allow us to make generalizable predictions about how people will behave in new environments. However, by construction, such models don't explain \emph{how} the mind achieves the rational ideal, and a growing list of systematic cognitive biases draw their predictive utility into question. 

In contrast, the \emph{mechanistic} approach focuses on identifying the cognitive processes underlying behavior, often with an emphasis on explaining the behavioral idiosyncrasies that rational models gloss over. This approach can potentially tell us how the mind actually works, and it can produce extremely accurate models. However, mechanistic models are often highly tuned to specific experimental setups---we are left wondering why this specific model fit data best, and whether it would continue to make good predictions in a slightly different environment.

Although the rational and mechanistic approaches have traditionally been viewed as conflicting, the past decade has seen a resurgence of an old idea \citep{simon1955behavioral}: rationality can be seen as a property of cognitive mechanisms themselves. Specificially, a cognitive mechanism is rational if it makes optimal use of limited cognitive resources. Going under various names---cognitively bounded rational analysis \citep{howes2009rational}, computational rationality \citep{lewis2014computational,gershman2015computational}, and resource-rational analysis \citep{griffiths2015rational,lieder2020} to name a few---this view suggests that we should not expect people to be rational in the traditional sense of selecting actions that maximize utility \citep{vonneumann1944theory}. Instead, we should expect people to select actions using mental strategies that strike a good tradeoff between the utility of the chosen action and the cogntive cost of making the decision.

But what defines a ``good'' tradeoff between action utility and cognitive cost? And how can we identify mental strategies that achieve such a tradeoff? Here, we suggest answers to these questions based on a key insight: a rational mental strategy is one that optimally solves the \textbf{sequential decision problem} posed by one's internal computational environment. Under this view, cognition is a problem of stringing together a series of basic cognitive operations or ``computations'' in the service of choosing which actions to take in the world. By formalizing this problem as a Markov decision process (MDP, the formalism underlying reinforcement learning), we can leverage standard tools from artificial intelligence to identify optimal cognitive processes. Doing so has already revealed a number of domains in which people's cognitive strategies are remarkably close to optimal, suggesting that the approach can provide accurate and generalizable predictions about human behavior. At the same time, people often show systematic deviations from the optimal model, suggesting ways that both our models and people's mental strategies might be improved.


\section{Optimal cognitive processes as solutions to Markov decision processes}

The proposed approach rests on a key intuition: the thoughts one has at any moment depend on the thoughts one had before. That is, our mental processes are sequentially dependent. Furthermore, thoughts are only useful insofar as they influence our behavior, and this behavior often occurs well after the thought itself. That is, the benefits of thought are temporally delayed. These two properties, sequential dependence and delayed reward make sequential decision problems very challenging to solve. Fortunately, a long history of work in artificial intelligence---from Newell and Simon's pioneering proof-writing programs\citep{newell1956logic} to super-human Chess and Go engines \citep{silver2017mastering}---has focused on solving just this sort of problem.

In artificial intelligence research, sequential decision problems are often formalized with the framework of \textbf{Markov decision processes (MDP)} \citep{puterman2014markov,sutton2018reinforcement}. An MDP describes a dynamic interaction between an agent and an environment. It is defined by a set of possible states the environment can be in, a set of actions the agent can execute, a reward function that specifies the immediate utility associated with executing each action in each state, and a transition function that specifies how actions change the state. The agent's goal is to maximize the total cumulative reward received. This can be accomplished by choosing actions according to the optimal policy, which specifies the best action to execute in each state. See Box~1 for a technical definition of MDPs.

The fact that cognition---or more generally, computation---poses a sequential decision problem was recognized by researchers in the field of \textbf{rational metareasoning}, which aims to build AI systems that can adaptively allocate their limited computational resources. In particular, Hay and colleagues \citealp{hay2012selecting,hay2016principles} formalize this problem of ``selecting computations'' as a \textbf{metalevel MDP}. In a metalevel MDP, the states correspond to beliefs and the actions correspond to computations that refine those beliefs (according to the transition function). The reward function encodes both the costs and benefits of computation; it assigns a strictly negative reward for each computation executed, but a potentially positive reward for the utility of the external action that is ultimately chosen (based on the belief produced by computation).

Applying the metalevel MDP formalism to cognitive science provides a suite of theoretical and computational tools, both to formalize the problems that our brains must solve, and to identify near-optimal solutions to those problems. In a psychological context, the states of a metalevel MDP can be interpreted as mental states, and the actions as cognitive operations. Along with the transition function, these specify the environment ``within the skin'' that a cognitive process must interact with. By further specifying a reward function, we can quantify the tradeoff between cognitive cost and extrinsic utility . This in turn allows us to identify the optimal cognitive process---that is, the one achieves the best possible tradeoff between cost and utility---as the optimal policy for the metalevel MDP.