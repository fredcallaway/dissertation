%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
We must be prepared to accept the possibility that what we call ``the environment'' may lie, in part, within the skin of the biological organism
\qauthor{Herbert Simon}
\end{savequote}

\chapter{Metalevel Markov decision processes}

\newthought{The key insight} underlying the proposed framework is that cognitive processes (including, for example, decision making) are themselves solutions to sequential decision problems. Drawing on a subfield of artificial intelligence known as \emph{rational metareasoning} \citep{matheson1968economic,russell1991principles}, we formalize this insight using the framework of \emph{meta-level Markov decision processes} (meta-level MDPs; \citealp{hay2012selecting}). In this framework, a cognitive process is formalized as a sequential process of executing computational actions that update an agent's beliefs about the world. At each moment, the agent must choose whether to continue deliberating, refining their beliefs but accruing computational cost, or to instead stop computing and make a decision. In the former case, they must additionally decide which computation to execute next (i.e., what to think about); in the latter case, they select the optimal action given their current belief and receive a reward associated with the external utility of that action.

In this chapter, I describe the formal framework and show how it can be applied to multi-attribute choice, the domain in which we conduct our experimental case studies. We provide a non-technical summary at the end of this section.

% We begin by introducing Markov decision processes (MDPs). Next, we define meta-level MDPs as extensions of standard MDPs. Finally, we define a specific meta-level MDP model for multi-attribute decision-making, which we will employ in our experimental case studies.

% Below, we give a high-level and intuitive overview of the general framework and its application to multi-attribute choice, the domain we will use in our case studies. A formal treatment is provided in Appendix~\ref{metamdp}.

% We begin by describing, in intuitive terms, how the framework can be applied to model multi-attribute decision-making. Next, we will formally define meta-level MDPs. Finally, we will return to the multi-attribute decision-making case, showing how the formalism can be applied to a specific case.

\begin{figure*}
  % \centering
  \includegraphics[width=\textwidth]{figs/metamdp.pdf}
  \caption{Formal framework: meta-level Markov decision processes.
  (A) A Markov decision process formalizes the problem of acting adaptively in a dynamic environment. The agent executes actions that change the state of the world and generate rewards, which the agent seeks to maximize.
  (B) A meta-level Markov decision process formalizes the problem of \emph{deciding how to act} when computational resources are limited. The agent executes computations that update their belief state and incur computational cost. When the agent executes the termination operation $\bot$, they take an external action based on their current belief state.}
  \label{fig:metamdp-diagram}
\end{figure*}


\section{Markov decision processes}

The core mathematical object underlying our approach is the Markov decision process (MDP), illustrated in Figure~\ref{fig:metamdp-diagram}A. MDPs are the standard formalism for modeling the sequential interaction between an agent and a stochastic environment. An MDP is defined by a set of states $\S$, a set of actions $\A$, a transition function $T$, and a reward function $r$. A state $s \in \S$ specifies the relevant state of the world. An action $a \in \A$ is an action the agent can perform. The transition function $T$ encodes the dynamics of the world as a distribution of possible future states for each possible previous state and action. Finally, the reward function $r$ specifies the reward or utility for executing a given action in a given state.

The standard goal in an MDP is to maximize the expected cumulative reward attained, that is, the \emph{return}. Importantly, this may require incurring immediate losses (negative rewards) in order to get to a state from which a highly rewarding action can be executed. It is typically assumed that the agent selects their actions based on the current state; the mapping from state to action is called a policy, denoted $\pi$. Solving an MDP amounts to finding a policy that maximizes the expected return, that is, a mapping from states to actions that, when followed, maximizes the total reward one will receive on average.

% An important property of MDPs is that there is at least one deterministic \emph{optimal policy}; that is, there is a mapping from states to actions that, when followed, will produce the maximum possible return. 

In addition to their foundational role in artificial intelligence \citep{sutton2018reinforcement}, MDPs are widely used in models of human decision-making \citep{dayan2008decision}. MDPs are the formal foundation for models of reinforcement learning \citep{niv2009reinforcement} and model-based planning \citep{huys2015interplay,botvinick2012planning}, as well as competition between the two systems \citep{daw2005uncertaintybased,keramati2011speed,kool2017costbenefit}. They have also been used to study information-seeking \citep{gottlieb2013informationseeking,hunt2016approachinduced}, generalization \citep{tomov2021multi}, and hierarchical abstraction \citep{solway2014optimal,botvinick2009hierarchically}. However, with a few notable exceptions \citep{dayan2008serotonin,drugowitsch2012cost,tajima2016optimal}, MDPs have primarily been used to model the sequential decision problems posed by the external world. In the following section, we show how this framework can be applied to model the sequential decision problem posed by one's own cognitive architecture.

\section{Meta-level Markov decision processes}

% The concept is thus very similar to \emph{elementary information processes} \citep{chase1978elementary,simon1979information,posner1982information,payne1988adaptive}. 

Meta-level Markov decision processes (meta-level MDPs) extend the standard MDP formalism to model the sequential decision problem posed by resource-bounded computation \citep{hay2012selecting}. Like a standard MDP, there is a set of states $\S$, a set of actions $\A$, and a reward function $r\object$ (we omit the transition function because we focus on one-shot decisions). These define the \emph{object-level} problem: the external problem the agent must solve in the world. Additionally, the meta-level MDP defines a set of beliefs $\B$, a set of computations $\C$, and meta-level transition and reward functions, $T\meta$ and $r\meta$. These define the \emph{meta-level} problem: how to allocate limited computational resources in the service of solving the object-level problem.

As illustrated in Figure~\ref{fig:metamdp-diagram}B, the meta-level problem is itself a sequential decision problem, analogous to one defined by a standard MDP. However, in the meta-level problem, the states are replaced by beliefs (mental states) and the actions are replaced by computations (cognitive operations). The meta-level transition function describes how computations update beliefs, and the meta-level reward function captures both computational cost and the object-level reward of the action that is ultimately executed. We provide a formal definition below.

We define a meta-level MDP as $(\S, \A, T\object, r\object, \B, \C, T\meta, r\meta)$. The first four components define the object-level problem. They have the same interpretation as $\S$, $\A$, $T$, and $r$ in a standard MDP. The latter four components define the meta-level problem. We now define these four components in turn.

\paragraph{Beliefs}
A belief state $b \in \B$ captures the agent's current knowledge about the relevant state of the world. Formally, a belief is a distribution states, $\B \subseteq \Delta(\S)$. Note that $\Delta(\S)$ denotes the set of all possible distributions over $\S$.
% One typically makes distributional assumptions that restrict the space of beliefs the agent can entertain, such that $\B \subset \Delta(\S)$. 
Importantly, contrary to a standard rational treatment of beliefs, the belief states in a meta-level MDP do not include all the information that is available to the DM. Instead, the belief state only contains information that is immediately accessible, excluding, for example, long-term memories and the number of calories in every box of cereal on a shelf.

\paragraph{Computations}
A computational operation $c \in \C$ is a primitive operation afforded by the computational architecture. Formally, it is a meta-level action that updates the belief in much the same way as a regular action changes state. All meta-level MDPs include the termination operation $\bot$, which denotes that computation should be terminated and an action should be selected based on the current belief state. We further explain belief updating and termination in the following two paragraphs.

\paragraph{Transition function}
The meta-level transition function $T\meta: \B \times \C \times \S \rightarrow \Delta(\B)$ describes how computation updates beliefs. At each time step, the next belief is sampled from a distribution that depends on the current belief, the computational operation that was just executed, and the true state of the world, that is,
\begin{equation}
b_{t+1} \sim T\meta(b_t, c_t, s).
\end{equation}
The transition function thus defines the core structure of the computational architecture. Following previous work \citep{Matheson1968,hay2012}, we assume that the effect of computation is to generate or reveal information about the true state of the world, which is then integrated into the belief state. Thus, in expectation, computation has the effect of making one's beliefs more precise and accurate, although an individual computation may yield misleading information. 

% This formalization is quite natural for many types of cognitive operations, such as memory recall and mental simulation. However, it can also be applied in less obvious cases; for example, performing an addition operation generates previously inaccessible information about the value of the sum of two numbers.

% the marginalized meta-level transition function is always stochastic. Without knowledge of the true object-level state $s$, the DM cannot predict exactly how a computation will change her belief---if she could, she could simply adopt that updated belief without performing the computation. 

\paragraph{Reward function}
The meta-level reward function $r\meta: \B \times \C \times \S \rightarrow \R$ describes both the costs and benefits of computation. For the former, $r\meta$ assigns a strictly negative reward for all non-terminating computational operations,
%
\begin{equation}
r\meta(b, c, s) = -\cost(c) \text{ for } c \neq \bot.
\end{equation}
%
The cost of computation may include multiple factors, such as energetic costs and opportunity costs. 

Intuitively, the benefit of computation is that it allows one to make better decisions. This is captured by the meta-level reward for the termination operation $\bot$, defined as the true utility of the external action that the DM would execute given the current belief. We assume that the action is selected optimally. Thus,
%
\begin{equation}\label{eq:term-reward}
r\meta(b, \bot, s) = r\object(s, a^*(b)).
\end{equation}
%
where
%
\begin{equation}\label{eq:object-action}
a^*(b) = \argmax_a \expect{r\object(s, a)}{s \sim b}
\end{equation}
%
In English, the meta-level reward for termination is the \emph{true} utility of the action\footnotemark{} with maximal \emph{estimated} utility.

\footnotetext{
  For notational clarity, we have assumed a single optimal action. When multiple actions have the same expected value, we assume that ties are broken randomly; thus, $a^*(b)$ is more precisely a uniform distribution over all optimal actions, and $r\meta(b, \bot, s)$ takes an expectation over them.
}

\paragraph{Policy}

The solution to a meta-level MDP takes the form of a policy $\pi: \B \rightarrow \Delta(\C)$ that (perhaps stochastically) selects which computation to perform in each possible belief state. The optimal policy is the one that maximizes expected meta-level return,
%
\begin{equation}
  \pi^* = \argmax_\pi \expect{
    \sum_{t=1}^T r\meta(B_t, C_t, S)
  }{
    C_t \sim \pi
  }.
\end{equation}
