%!TEX root = ../dissertation.tex
\Chapter{Conclusion}
\label{conclusion}


\newthought{In this disseration}, we have introduced a general framework for modeling cognition as a sequential decision problem: metalevel Markov decision processes. We then showed how the framework can be applied to derive rational mechanistic models in three different domains: attention, memory, and planning. In each case, we found that human behavior showed substantial qualitative alignment with the optimal metalevel policy. Taken together, the results suggest that human cognitive processes are well-adapted to the internal environments in which they operate. However, in no case did we find perfect correspondence between humans and the optimal policy. Although some of these discrepancies could be explained by the many simplifying assumptions we made, it is likely that people's cognitive processes are not perfectly adaptive. Understanding the source of these deviations may be outside the scope of the framework.

The breadth of domains covered in Chapters~\ref{sec:attention}-\ref{sec:planning} is not insignificant, but the models share many core features. Although Chapters~\ref{sec:attention} and~\ref{sec:memory} considered very different tasks (attention in choice and memory recall), they relied on essentially the same state space and transition structure, based on Gaussian evidence accumulation. This parallels the breadth of domains in which evidence accumulation models have been applied. Our model of planning (Chapter~\ref{sec:planning}) employed a very different type of state space (decision trees), it shares with Chapter~\ref{sec:attention} the idea that decision-making can be understood as gathering information about rewards (c.f. \citealp{tajima2016optimal,sezener2019optimizing}). All three chapters rely on probalistic models that specify how the effects of computations relate to an unknown world state. Although these similarities undercut the claimed generality of the approach (see TODO), they can also be viewed as a strength. By specifying models using a common framework, it is easier to transfer concepts and computational tools between domains.

\section{Kindred efforts}

The idea of modeling cognitive processes as optimal solutions to sequential decision problems is not unique to this dissertation. There are at least three major strands of research in this area, which we briefly review below.

\subsection{Optimal evidence accumulation}

As mentioned in the introduction, there is a long history of modeling optimal speed-accuracy tradeoffs using evidence accumulation models. Early work (e.g., \citealp{bogacz2006physics,vul2009one}) focused on binary choices with accuracy-based rewards (e.g., +1 for correct responses) and known evidence coherence (i.e., the strength of evidence supporting the correct response is always the same). In these cases, the optimal stopping rule is given \emph{sequential probability ratio test} (SPRT): continue collecting evidence until a threshold level of evidence is reached either for or against the hypothesis. However, when any of these assumptions are violated, the SPRT (and by extension) is no longer optimal.

By explicitly modeling evidence accumulation as a sequential decision problem, \citet{drugowitsch2012cost} were able to derive the optimal stopping rule when evidence coherence varies from trial to trial.\footnote{%
  This problem is more complex because one cannot simply sum up the log likelihoods for each piece of evidence (which could support option A or B). One must maintain a full belief state over the drift rate, which specifies both the correct answer and the coherence (higher absolute values correspond to higher coherence). 
} This model adopts the same basic evidence accumulation and belief updating dynamics as in Chapter~\ref{sec:attention} and~\ref{sec:memory}, with a single accumulator tracking relative evidence for one choice vs. the other. They showed that the optimal policy corresponds to a threshold that changes over time (rapidly expanding and then slowly collapsing). Building on this work, \citet{tajima2016optimal} characterized the optimal decision thresholds for value-based choices, where both importance and difficulty depend on the difference in value between the choice options (c.f. \citealp{fudenberg2018speed}). \citet{tajima2019optimal} chararcterize the optimal solution for three-alternative choices, where the mental states and thresholds reside in an three-dimensional space (including time).

Despite this richness, all the models reviewed so far have assumed a very simple cognitive architecture, in which there are only two possible cognitive operations: gather more evidence, or stop. Building on the Tajima et al. model, \citet{jang2021optimal} derived the optimal policy for the case when the agent can only sample from one item at a time (more precisely, when one the value of the attended item produces less noisy samples). Without switching costs, \citet{fudenberg2018speed} showed that it is optimal to perfectly balance attention to each item (alternating at each time step), but in the presence of switching costs, the optimal policy takes on a more interesting structure. As we showed in Chapter~\ref{sec:attention}, the optimal policy takes on even richer structure when there are more than two alternatives, preferentially attending to items with high estimated value. However, the high dimensionality of the state space makes it impossible to exactly identify the optimal policy for ternary choice using backwards induction (the method employed in the Drugowitsch, Tajima, and Jang papers; described in Section~\ref{sec:backinduct}). We were only able to (approximately) identify the optimal policy in this case using the BMPS algorithm (Section~\ref{sec:bmps}), which we developed specifically to solve metalevel MDPs. This highlights the value of using a formalism that is tailored to the specific type of sequential decision problems posed by cognition.

Another line of work in economics has also sought to characterize optimal evidence accumulation in cases where the agent can control the nature of evidence sampled \citep{woodford2014stochastic,hebert2017rational}. The key distinguishing feature of these models is the assumption of a very flexible cognitive architecture in which the agent can gather an arbitrary form evidence at each time step (formally a conditional distribution of a signal given the true world state). Paradoxically, allowing for such flexibility actually makes it easier\footnote{%
  Assuming you have the math training of an Econ PhD.
} to identify the optimal policy, as it can be derived analytically. However, this flexibility may limit the ability of the model to account for human cognitive processes, which are likely forced to adapt to a more restrictive architecture.

\subsection{POMDP}\label{sec:alternative-pomdp}


\section{Points of weakness and avenues for growth}

There are several dimensions on which the framework could be extended in future work.

\subsection{Educating the metalevel homunculus}

One of the most significant challenges for the metalevel MDP framework is the problem of infinite regress. As mentioned in the Introduction (Section~\ref{sec:bound-meta}), the framework assumes that people are \emph{metalevel rational}, meaning that they choose computational actions to optimally balance a cost-benefit tradeoff. However, it does not explain how those choices are themselves made. In this way, the framework assumes a ``metalevel homonculus'': an unbounded, perfectly rational agent that always knows just what thought to think next (c.f. \citealp{hazy2006banishing,botvinick2014computational}). Explaining (or perhaps explaining away) the homonculus is critical for these models to provide a complete explanation of how people effectively allocate computational resources.

Perhaps the most plausible theory of the metalevel homonculus is that it is learned. Indeed, learning has been a critical aspect of many models of metalevel control in psychology. The earliest examples are found in cognitive architectures like ACT-R and SOAR, which have simple mechanisms for learning when to apply different production rules \citep{laird1986chunking}. However, like early models of strategy selection \citep{shrager1998scads}, this type of learning was primiarily associative. The idea that a metalevel controller explicitly learns to maximize reward was proposed in later strategy selection models \citet{erev2005adaptation,rieskamp2006ssl,lieder2017strategy}. Reinforcement learning (RL) has also been proposed as the mechanism by which the brain learns which goals to pursue \citep{cushman2015habitual}, and when to retrieve and store information in working memory \citep{oreilly2006making,todd2008learning} and episodic memory \citep{lu2022neural}. 

Metalevel MDPs provide a natural framework in which to cast theories of metacognitive RL. However, expliciting formalizing the metalevel control problem in this way also reveals a major challenge: learning good policies in metalevel MDPs is very challenging. They tend to have very large (mental) state spaces, and they have highly stochastic dynamics. Perhaps most challenging, however, is the temporal credit assignment problem. In each episode, the agent takes many computations, but receives only a single external reward. It is thus not clear which computations should receive ``credit'' for large rewards. 

One way to make learning easier in the presence of sparse rewards is to adjust the reward function, adding \emph{shaping rewards} (or ``pseudo rewards'') that provide more immediate feedback about the value of each (mental) action that is executed \citep{ng1999policy}. We have shown that shaping rewards derived from the optimal metalevel value functions can accelerate human learning in the Mouselab MDP task used in Chapter~\ref{sec:planning} \citep{callaway2022leveraging}. Although people would not have access to such perfect shaping rewards in the real world, they may have access to simpler but still useful surrogates.\footnote{%
  \citet{hay2016principles} propose one intuitively appealing shaping reward based on the difference in the expected value of taking an external action in the mental states before and after executing a computation. However, the results were, in his terms, ``not yet as good as we’d like''.
} That is, people may experience a reward when they have a ``good thought'' even if it does not immediately lead to an external action \citep{gopnik1998explanation}. Indeed, people appear to place value on external information that cannot be acted on \citep{eliaz2007experimental,gottlieb2018neuroscience}, and we have begun to formalize the specific qualities of information that people value \citep{markant2014preference,markant2016selfdirected}. This suggests a fascinating research question: what factors elicit the subjective experience of having a good thought?


\subsection{Partially observable minds}

% A key assumption of the metalevel DMP framework is that the agent has direct access to the mental state. Indeed, 

A key structural assumption in the framework is the distinction between the mental state and the world state. The defining features of the mental state are that it can be affected by computation, and that it is directly accessible to the agent. However, these two features do not need to come together. It is possible that we do not have complete access to aspects of our mental state that we can nevertheless control to some extent. This possibility was suggested by \citet{suchow2016deciding}, who proposed a POMDP model of working memory maintenance. In this model, the agent selects mental actions to increase the activation level of a selected memory, but the current activation levels can only be imperfectly measured. This model did not invoke a notion of world states or external actions, however.

To allow for partial observability in a metalevel MDP, we can simply assume that the agent does not have direct access to the mental state, but instead has access to an incomplete observation of that state. An interesting question arises as to how these observations should be used. From a POMDP perspective, the observations should be integrated over time into a metalevel belief state (a distribution over mental states). However, this complicates the metalevel controller considerably, creating technical challenges in finding optimal solutions, and exacerbating the homonculus problem described above. An alternative approach, employed by \citet{suchow2016deciding}, is to assume that the metalevel decisions are made based only on the current observation. This yields a simpler and perhaps more psychologically plausible model. However, because observations are not Markovian, one could no longer use standard dynamic programming techniques to find the optimal policy. This further motivates the development of model-free strategies for learning metalevel policies.

\begin{figure*}
  \centering
  \includegraphics[width=0.8\textwidth,page=2,trim=0 100 0 50]{diagrams/metamdp.pdf}
  % \includegraphics[width=\textwidth]{figs/metamdp.pdf}
  \caption{\captiontitle{Interleaved metalevel MDP}. To formalize the problem of interleaved computation and action, we can augment the metalevel MDP framework to include sequences of world states, actions, and external rewards. Note that there is no cognitive cost in this formalism, as any opportunity costs should be captured in the external reward function. The elements that capture the external environment are indicated in blue.
  }
  \label{fig:metamdp-joint}
\end{figure*}

\subsection{Interleaved computation and action}

A second key structural assumption in the framework is that each metalevel episode occurs within a single external timestep. That is, the world state does not change, and only one external action is executed (at the end of the episode). This has two implications. First, it assumes that the agent can think as long as they want without having to worry about the world state changing. Second, it assumes that information considered while choosing one action cannot influence the selection of future actions. Although these assumptions hold approximately in many experimental and naturalistic tasks, they will not hold in fast-paced dynamic problems (e.g. jay walking) or in sequential problems with stochastic dynamics (e.g. chess).

To model interleaved computation and action, we essentially replace the single world state and action with a full external MDP that progresses in lockstep with the metalevel MDP (c.f. joint-state MDPs; \citealp{russell1991right,parr1998reinforcement,hay2016principles}). Figure~\ref{fig:metamdp-joint} illustrates one way this could work. At each timestep the agent selects both a computation and an action (the action may be to simply sit still while thinking). The next world state and reward depend on the current state and action, as in a standard MDP. As before, the next mental state depends on the current mental state and the computation executed; it additionally depends on the previous action and the \emph{new} world state. The former captures the fact that the agent may understand how their actions affect the world. The latter captures the agent's ability to perceive the changing world state.

In the interleaved case, the decision about how much to think becomes more nuanced. Specifically, one must not only decide \emph{how much} to think but also \emph{when} to think. In some cases, it may make sense to do all of one's thinking up front, as the standard metalevel MDP assumes. This strategy makes sense because it ensures that every action you take is informed by all the computation you do. However, there are at least three reasons one might want to begin acting before having a complete plan. First, one may be able to continue thinking while executing the early part of the plan, for example considering which way one will turn while walking down a long corridor \citep{oceallaigh2015metareasoning}. Second, if the world dynamics are stochastic, knowing how those transitions unfold will allow the agent to focus their planning on the the situtation the agent actually encounters. Third, forming a complete plan may impose representational costs that could be avoided by only constructing a concrete plan for the immediate future, perhaps having a more abstract plan for the more distant future \citep{ho2020efficiency}.

\subsection{Optimizing the architecture}

A third assumption of the framework is that only the policy, and not the metalevel MDP itself, is optimized. This assumption is almost always made in standard applications of MDPs, as the MDP represents the external environment, which the agent has no direct control over. In contrast, beccause a metalevel MDP represents an agent's internal computational environment, it is likely that the metalevel MDP itself is adapted to the structure of the external problems the agent has to solve.

There are two timescales at which adaptation of an organism's cognitive architecture could occur (in animals): evolutionary and developmental. Clearly, evolution has a major role in shaping the total amount of biological resources allocated to cognition (e.g., brain volume), as well as the ease with which certain types of computations can be learned and executed. However, at the level of abstraction that we have posed metalevel MDPs, the developmental timescale is likely to be more relevant. Indeed, it is natural to view many types of learning as a process of developing new mental states and computational actions. For example, as a consequence of reading this dissertation, you have (hopefully) acquired a computation along the lines of ``identify the computational actions in this cognitive model''.

\subsection{Renouncing the reverend}  % Abandoning Bayes


\section{Parting words}



